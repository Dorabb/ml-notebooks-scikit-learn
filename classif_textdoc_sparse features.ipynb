{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPWh7XIbJ4k78D6pVk6oExu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Classification of text documents using sparse features"],"metadata":{"id":"gV90x5l9ftBM"}},{"cell_type":"code","source":["#classify documents by topics using a Bag of words approach\n","#uses a Tf-idf-weighted document-term sparse matrix to encode the features and demonistrate classifiers that can be handled sparse matrices.\n","\"\"\"\n","Load and vectorizing thye 20 newsgroups text dataset\n","18000 newsgroups post on 20 topics in two subsets 1. for training\n","                                                  2.for testing\n","\"\"\"\n","from time import time\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","categories = [\n","    \"alt.atheism\",\n","    \"talk.religion.misc\",\n","    \"comp.graphics\",\n","    \"sci.space\",\n","]\n"],"metadata":{"id":"AEzmoNEof0UR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def size_mb(docs):\n","  return sum(len(s.encode(\"utf-8\")) for s in docs) / 1e6"],"metadata":{"id":"brZYbT4FiBDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_dataset(verbose=False, remove=()):\n","  \"\"\" Load and vectorize the 20 newsgroups dataset.\"\"\"\n","  data_train = fetch_20newsgroups(\n","    subset=\"train\",\n","    categories=categories,\n","    shuffle=True,\n","    random_state=42,\n","    remove=remove,\n","  )\n","  data_test = fetch_20newsgroups(\n","      subset=\"test\",\n","      categories=categories,\n","      shuffle=True,\n","      random_state=42,\n","      remove=remove,\n","  )\n","\n","  #order of lables in targes_names can be diffrent from categories\n","  target_names = data_train.target_names\n","\n","  # split target in a training set and a test set\n","  y_train,y_test = data_train.target, data_test.target\n","\n","  # Etracting featuresfrom the train ing data using a sparse vectorizer\n","  t0 = time()\n","  vectorizer = TfidfVectorizer(\n","      sublinear_tf=True, max_df=0.5, stop_words=\"english\"\n","  )\n","  X_train = vectorizer.fit_transform(data_train .data)\n","  duration_train = time() - t0\n","\n","  # Extracting features from the test data using same vectorizer\n","  t0 = time()\n","  X_test = vectorizer.transform(data_test.data)\n","  duration_test = time() - t0\n","\n","  feature_names = vectorizer.get_feature_names_out()\n","  if verbose:\n","    #compute size of loaded data\n","    data_train_size_mb = size_mb(data_train.data)\n","    data_test_size_mb = size_mb(data_test.data)\n","\n","    print(\n","        f\"{len(data_train.data)} documents - {data_train_size_mb:.2f}MB (training set)\"\n","    )\n","    print(\n","        f\"{len(data_test.data)} documents - {data_test_size_mb:.2f}MB (test set)\")\n","    print(f\"{len(target_names)} categories\")\n","    print(\n","        f\"vectorize training done in {duration_train:.3f}s \"\n","        f\" at {data_train_size_mb / duration_train:.3f}MB/s\"\n","    )\n","    print(f\"n_samples: {X_train.shape[0]}, n_features: {X_train.shape[1]}\")\n","    print(\n","        f\"vectorize testing done in {duration_test:.3f}s \"\n","        f\"at {data_test_size_mb / duration_test:.3f}MB/s\"\n","    )\n","    print(f\"n_samples: {X_test.shape[0]}, n_features: {X_test.shape[1]}\")\n","\n","  return X_train, X_test, y_train, y_test, feature_names, target_names\n","\n","X_train, X_test, y_train, y_test, featurenames, target_names = load_dataset(\n","    verbose=True\n",")"],"metadata":{"id":"nkg8v9ZziaJH"},"execution_count":null,"outputs":[]}]}